{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894ccac",
   "metadata": {},
   "source": [
    "# Data Manipulation Studio\n",
    "\n",
    "For this studio, we will revisit our California farmers looking for advice on growing pumpkins and the same [pumpkins dataset](https://www.kaggle.com/usda/a-year-of-pumpkin-prices) as provided by the U.S. Department of Agriculture. You may have to clean data in the process of data manipulation, so feel free to pull up your notebook from the last class's studio.\n",
    "\n",
    "We will now be focusing our attention on a different region in the United States, the Northeast. When you open up the `dataset` folder, you will have 13 CSVs, including the San Francisco and Los Angeles data from the last lesson. The 13 CSVs are each a different terminal market in the United States.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Import the CSVs for each of the following cities: Baltimore, Boston, New York, and Philadelphia. Set up a dataframe for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c9a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and CSVs. Make some dataframes!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "# dt_altana=pd.read_csv(\"atlanta_9-24-2016_9-30-2017.csv\")\n",
    "dt_baltimore=pd.read_csv(\"baltimore_9-24-2016_9-30-2017.csv\")\n",
    "dt_boston=pd.read_csv(\"boston_9-24-2016_9-30-2017.csv\")\n",
    "# dt_chicago=pd.read_csv(\"chicago_9-24-2016_9-30-2017.csv\")\n",
    "# dt_columbia=pd.read_csv(\"columbia_9-24-2016_9-30-2017.csv\")\n",
    "# dt_dallas=pd.read_csv(\"dallas_9-24-2016_9-30-2017.csv\")\n",
    "# dt_detroit=pd.read_csv(\"detroit_9-24-2016_9-30-2017.csv\")\n",
    "# dt_los_angeles=pd.read_csv(\"los-angeles_9-24-2016_9-30-2017.csv\")\n",
    "# dt_miami=pd.read_csv(\"miami_9-24-2016_9-30-2017.csv\")\n",
    "dt_new_york=pd.read_csv(\"new-york_9-24-2016_9-30-2017.csv\")\n",
    "dt_philadelphia_=pd.read_csv(\"philadelphia_9-24-2016_9-30-2017.csv\")\n",
    "# dt_sanfranssico=pd.read_csv(\"san-fransisco_9-24-2016_9-30-2017.csv\")\n",
    "# dt_st_louis =pd.read_csv(\"st-louis_9-24-2016_9-30-2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfda42f",
   "metadata": {},
   "source": [
    "## Clean Your Data\n",
    "\n",
    "In the last lesson, we cleaned the data to related to San Francisco. Pull up your notebook from the last lesson and use your cleaning skills to clean the dataframes as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98abc290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commodity Name - 0%\n",
      "City Name - 0%\n",
      "Type - 100%\n",
      "Package - 0%\n",
      "Variety - 1%\n",
      "Sub Variety - 84%\n",
      "Grade - 100%\n",
      "Date - 0%\n",
      "Low Price - 0%\n",
      "High Price - 0%\n",
      "Mostly Low - 0%\n",
      "Mostly High - 0%\n",
      "Origin - 3%\n",
      "Origin District - 100%\n",
      "Item Size - 16%\n",
      "Color - 80%\n",
      "Environment - 100%\n",
      "Unit of Sale - 84%\n",
      "Quality - 100%\n",
      "Condition - 100%\n",
      "Appearance - 100%\n",
      "Storage - 100%\n",
      "Crop - 100%\n",
      "Repack - 0%\n",
      "Trans Mode - 100%\n"
     ]
    }
   ],
   "source": [
    "# Clean your data here!\n",
    "for col in dt_baltimore.columns:\n",
    "    pct_missing = np.mean(dt_baltimore[col].isnull())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb696ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commodity Name - 0%\n",
      "City Name - 0%\n",
      "Type - 100%\n",
      "Package - 0%\n",
      "Variety - 0%\n",
      "Sub Variety - 92%\n",
      "Grade - 100%\n",
      "Date - 0%\n",
      "Low Price - 0%\n",
      "High Price - 0%\n",
      "Mostly Low - 0%\n",
      "Mostly High - 0%\n",
      "Origin - 0%\n",
      "Origin District - 81%\n",
      "Item Size - 1%\n",
      "Color - 14%\n",
      "Environment - 100%\n",
      "Unit of Sale - 87%\n",
      "Quality - 100%\n",
      "Condition - 100%\n",
      "Appearance - 100%\n",
      "Storage - 100%\n",
      "Crop - 100%\n",
      "Repack - 0%\n",
      "Trans Mode - 100%\n"
     ]
    }
   ],
   "source": [
    "for col in dt_boston.columns:\n",
    "    pct_missing = np.mean(dt_boston[col].isnull())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7013ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commodity Name - 0%\n",
      "City Name - 0%\n",
      "Type - 100%\n",
      "Package - 0%\n",
      "Variety - 0%\n",
      "Sub Variety - 84%\n",
      "Grade - 100%\n",
      "Date - 0%\n",
      "Low Price - 0%\n",
      "High Price - 0%\n",
      "Mostly Low - 0%\n",
      "Mostly High - 0%\n",
      "Origin - 0%\n",
      "Origin District - 87%\n",
      "Item Size - 7%\n",
      "Color - 81%\n",
      "Environment - 100%\n",
      "Unit of Sale - 78%\n",
      "Quality - 100%\n",
      "Condition - 100%\n",
      "Appearance - 100%\n",
      "Storage - 100%\n",
      "Crop - 100%\n",
      "Repack - 0%\n",
      "Trans Mode - 100%\n"
     ]
    }
   ],
   "source": [
    "for col in dt_new_york.columns:\n",
    "    pct_missing = np.mean(dt_new_york[col].isnull())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f487f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commodity Name - 0%\n",
      "City Name - 0%\n",
      "Type - 100%\n",
      "Package - 0%\n",
      "Variety - 0%\n",
      "Sub Variety - 79%\n",
      "Grade - 100%\n",
      "Date - 0%\n",
      "Low Price - 0%\n",
      "High Price - 0%\n",
      "Mostly Low - 0%\n",
      "Mostly High - 0%\n",
      "Origin - 0%\n",
      "Origin District - 100%\n",
      "Item Size - 21%\n",
      "Color - 100%\n",
      "Environment - 100%\n",
      "Unit of Sale - 81%\n",
      "Quality - 100%\n",
      "Condition - 100%\n",
      "Appearance - 100%\n",
      "Storage - 100%\n",
      "Crop - 100%\n",
      "Repack - 0%\n",
      "Trans Mode - 100%\n"
     ]
    }
   ],
   "source": [
    "for col in dt_philadelphia.columns:\n",
    "    pct_missing = np.mean(dt_philadelphia[col].isnull())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd74659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.0 % Missing cells from the data\n",
      "22.0 % Missing cells from the data\n",
      "51.0 % Missing cells from the data\n",
      "49.0 % Missing cells from the data\n"
     ]
    }
   ],
   "source": [
    "# missing % for baltimore\n",
    "total_cells_bal = np.product(dt_baltimore.shape)\n",
    "\n",
    "missing_cells_bal = pd.isnull(dt_baltimore).sum()\n",
    "\n",
    "total_missing_bal = missing_cells_bal.sum()\n",
    "\n",
    "percentage_missing_bal = round((total_missing_bal/total_cells_bal), 2) *100\n",
    "\n",
    "print(percentage_missing_bal, \"% Missing cells from the data\") \n",
    "# missing % for boston\n",
    "total_cells_bo = np.product(dt_boston.shape)\n",
    "\n",
    "missing_cells_bo = pd.isnull(dt_boston).sum()\n",
    "\n",
    "total_missing_bo = missing_cells_bo.sum()\n",
    "percentage_missing_bo = round((total_missing/total_cells_bo), 2) *100\n",
    "\n",
    "print(percentage_missing_bo, \"% Missing cells from the data\") \n",
    "\n",
    "# missing % of philephidia\n",
    "\n",
    "total_cells_phili = np.product(dt_philadelphia.shape)\n",
    "\n",
    "missing_cells_phili = pd.isnull(dt_philadelphia).sum()\n",
    "\n",
    "total_missing_phili = missing_cells_phili.sum()\n",
    "percentage_missing_phili = round((total_missing_phili/total_cells_phili), 2) *100\n",
    "\n",
    "print(percentage_missing_phili, \"% Missing cells from the data\") \n",
    "\n",
    "# missing % of newyork\n",
    "total_cells_ny = np.product(dt_new_york.shape)\n",
    "\n",
    "missing_cells_ny = pd.isnull(dt_new_york).sum()\n",
    "\n",
    "total_missing_ny = missing_cells_ny.sum()\n",
    "percentage_missing_ny = round((total_missing_ny/total_cells_ny), 2) *100\n",
    "\n",
    "print(percentage_missing_ny, \"% Missing cells from the data\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f26459ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 24)\n",
      "(352, 24)\n",
      "(57, 24)\n",
      "(112, 24)\n"
     ]
    }
   ],
   "source": [
    "dt_baltimore_droping= dt_baltimore.drop(labels=[\"Color\"],axis=1,inplace=False)\n",
    "dt_baltimore_droping\n",
    "print(dt_baltimore_droping.shape)\n",
    "\n",
    "dt_boston_droping= dt_boston.drop(labels=[\"Color\"],axis=1,inplace=False)\n",
    "dt_boston_droping\n",
    "print(dt_boston_droping.shape)\n",
    "\n",
    "dt_philadelphia_droping= dt_philadelphia.drop(labels=[\"Color\"],axis=1,inplace=False)\n",
    "dt_philadelphia_droping\n",
    "print(dt_philadelphia_droping.shape)\n",
    "\n",
    "dt_new_york_droping= dt_new_york.drop(labels=[\"Color\"],axis=1,inplace=False)\n",
    "dt_new_york_droping\n",
    "print(dt_new_york_droping.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267e574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e70b01b9",
   "metadata": {},
   "source": [
    "## Combine Your Data\n",
    "\n",
    "Now that you have four clean sets of data, combine all four into one dataframe that represents the entire Northeast region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da059f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt_philadelphia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine the four dataframes into one!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_frames\u001b[38;5;241m=\u001b[39m[dt_baltimore,dt_boston,\u001b[43mdt_philadelphia\u001b[49m,dt_new_york]\n\u001b[1;32m      3\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m  left,right: pd\u001b[38;5;241m.\u001b[39mmerge(left,right,on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommodity Name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                             how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m), data_frames)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_merged\u001b[38;5;241m.\u001b[39mshape()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dt_philadelphia' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine the four dataframes into one!\n",
    "data_frames=[dt_baltimore,dt_boston,dt_philadelphia,dt_new_york]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['Commodity Name'],\n",
    "                                            how='outer'), data_frames).fillna('void')\n",
    "df_merged.shape()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590082f",
   "metadata": {},
   "source": [
    "## Answer Some Questions\n",
    "\n",
    "Use `groupby()` and `agg()` to answer the following two questions:\n",
    "\n",
    "1. What is the mean low and high prices for each type of unit of sale in the Northeast region? In the last lesson, we learned that a unit of sale could be something like a bin or individually. \n",
    "2. What is the average number of pumpkins for each variety that came into terminal markets for the year by region? Pumpkin varieties include Howden and Fairytale pumpkins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c839639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here to find the mean low and high prices in the Northeast region for each type of unit of sale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4b23352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here to find the average number of pumpkins coming into terminal markets of each variety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5cff4",
   "metadata": {},
   "source": [
    "## Bonus Mission\n",
    "\n",
    "Try answering the same questions for the Midwest (Chicago, Detroit, and St. Louis) or the Southeast (Atlanta, Columbia, and Miami) regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d22b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the bonus mission if you have time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbc152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
